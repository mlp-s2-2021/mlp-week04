{"cells":[{"cell_type":"markdown","source":"# Machine Learning in Python - Workshop 4","metadata":{"cell_id":"00000-6b2b909d-de62-4725-b77a-4f7918b2f5a6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 1. Setup\n\n### 1.1 Packages\n\nIn the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. ","metadata":{"cell_id":"00001-2c500e66-1ebe-4eb7-8891-fa453665d1bb","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00002-be3af6fa-4647-4f8a-9ed8-40a8c51fb0ff","output_cleared":true,"deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline\n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotting defaults\nplt.rcParams['figure.figsize'] = (8,5)\nplt.rcParams['figure.dpi'] = 80\n\n# sklearn modules\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Data\n\nTo start we will again be using the same data set from Workshop 3, which was generated via a random draw from a Gaussian Process model. It represent an unknown smooth function $f(x)$ that is observed with noise such that $y_i = f(x_i) + \\epsilon_i$. The data have been randomly thinned to include only 100 observations.\n\nWe can read the data in from `gp.csv` and plot the data to see the overall trend in the data,","metadata":{"cell_id":"00003-bd435956-0c3e-410f-8f68-8716dadde273","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00004-c81f84a4-a191-49e1-9b96-ada4a1b11f39","output_cleared":true,"deepnote_cell_type":"code"},"source":"d = pd.read_csv(\"gp.csv\")\nn = d.shape[0] # number of rows\n\nsns.scatterplot(x='x', y='y', data=d, color=\"black\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Cross validation\n\nIn this section we will explore some of the tools that sklearn provides for cross validation for the purpose of model evaluation and selection. The most basic form of CV is to split the data into a testing and training set, this can be achieved using `train_test_split` from the `model_selection` submodule. Here we provide the function with our model matrix $X$ and outcome vector $y$ to obtain a test and train split of both.","metadata":{"cell_id":"00005-ae22b630-ebc4-4001-9ec0-12f2556dd3b9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00006-8851c86b-4b81-4e35-bed2-9883cea07dfe","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.model_selection import train_test_split\n\nX = np.c_[d.x]\ny = d.y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The additional arguments `test_size` determines the proportion of data to include in the test set and `random_state` is the seed used when determining the partition assignments (keeping the seed the same will result in the same partition(s) each time this cell is rerun).\n\nWe can check the dimensions of the original and new objects using their shape attributes,","metadata":{"cell_id":"00007-d3cde2db-08cd-44fc-ac97-f57c7b556e98","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00008-7e6d65e9-ae76-4e09-b146-0e3f2e88f304","output_cleared":true,"deepnote_cell_type":"code"},"source":"print(\"orig sizes :\", X.shape, y.shape)\nprint(\"train sizes:\", X_train.shape, y_train.shape)\nprint(\"test sizes :\", X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these new objects we can try several polynomial regression models, with different values of `M`, and compare their performance. Our goal is to fit the models using the training data and then evaluating their performance using the test data so that we can avoid potential overfitting.\n\nWe will assess the models' performance using root mean squared error, \n\n$$ \\text{rmse} = \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\right)^{1/2} $$\n\nwith sklearn this is calculated using the `mean_squared_error` function with the argument `squared=False`. This metric is entirely equivalent to mean squared error for purposes of ranking / ordering models (as the square root is a monotonic transformation) but the rmse is often prefered as it is more interpretable, as it has the same units as $y$.\n\nThe following code uses a `for` loop to fit 30 polynomial regression models with $M = [1,2,\\ldots,30]$ and calculates the rmse of the training data and the testing data for each model. Note that we are fitting the model *only* using the `train` split and predicting using both the `train` and the `test` split to get the train and test rmses respectively.","metadata":{"cell_id":"00009-1a0d3b31-8cd8-4457-b3d0-82fb88370bce","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00010-85d76a25-dba7-4875-801d-7fbbbc809899","output_cleared":true,"deepnote_cell_type":"code"},"source":"degree = []\ntrain_rmse = []\ntest_rmse = []\n\nM = 30\n\nfor i in np.arange(1, M+1):\n    m = make_pipeline(\n        PolynomialFeatures(degree=i),\n        LinearRegression(fit_intercept=False)\n    ).fit(X_train, y_train)\n    \n    degree.append(i)\n    train_rmse.append(mean_squared_error(y_train, m.predict(X_train), squared=False))\n    test_rmse.append(mean_squared_error(y_test, m.predict(X_test), squared=False))\n\nfit = pd.DataFrame(data = {\"degree\": degree, \"train_rmse\": train_rmse, \"test_rmse\": test_rmse})","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then plot `degree` vs `rmse` for these splits and observe the resulting pattern.","metadata":{"cell_id":"00011-af5e2e4f-de74-46a0-b5bc-efd7ccb1b5a9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00012-2a00fb71-ed7b-46d1-b4b6-fe299311cea4","output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.lineplot(x=\"degree\", y=\"value\", hue=\"variable\", data = pd.melt(fit,id_vars=[\"degree\"]))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 1\n\nBased on these results, what value of $M$ produces the best model, justify your answer.","metadata":{"cell_id":"00013-02586468-5fb0-439c-b6b9-92b4400cd0ec","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00014-83186210-d6a1-471f-bb1d-a3f741b5531f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 2\n\nTry adjusting the proportion of the data in the test vs training data, how does this change the Training and Testing rmse curves?","metadata":{"cell_id":"00015-da18e360-f2c1-466e-becf-5db18f897521","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00016-22057ba7-0790-4421-868e-a841e41147dc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 2.1 k-fold cross validation\n\nThe basic implementation of k-fold cross validation is provided by `KFold` in the `model_selection` submodule of `sklearn`. This function / object behaves in a similar way to the other `sklearn` tools we've seen before, we use the function to construct an object with some basic options and the resulting object then can be used to interact with our data / model matrix.","metadata":{"cell_id":"00017-3fe5ee3d-137d-4628-af61-7f99abd30a1b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00018-311ded87-ebac-4e55-8f5b-52c258f832b1","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00019-bcb249e3-613c-406f-8d27-7d3208bfc5ce","output_cleared":true,"deepnote_cell_type":"code"},"source":"kf = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have set up the object `kf` to implement 5-fold cross validation for our data, which we can then apply using the `split` method on our model matrix `X` and response vector `y`.","metadata":{"cell_id":"00020-6ed60165-fc48-46f9-b175-ca65386ed0f1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00021-00980725-e830-40c5-a03b-c405027bbe8f","output_cleared":true,"deepnote_cell_type":"code"},"source":"kf.split(X, y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This returns a [generator](https://wiki.python.org/moin/Generators) which is then used to generate the indexes of the training data and test data for each fold. We can use list comprehension with this generator to view these values.","metadata":{"cell_id":"00022-c16ff884-b77c-4ab8-8df4-f5ecfc62894e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00023-a4bde1e2-a0bd-4477-8020-3fcfd6948ca2","output_cleared":true,"deepnote_cell_type":"code"},"source":"[train for train, test in kf.split(X, y)]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00024-8cb8140d-df07-4da3-99ca-a37c09d2e429","output_cleared":true,"deepnote_cell_type":"code"},"source":"[test for train, test in kf.split(X, y)]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 3\n\nExamine the index values that make up the test and training sets, do you notice anything intesting about how `KFold` has divided up the data?","metadata":{"cell_id":"00025-3f00282f-aaec-44f2-b481-57752d0fd5a0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00026-cf05e4bb-5b1b-4dcf-a328-5770dd5bcb4e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 4\n\nDoes the structure in Exercise 3 have any implications for model evaluation? Specifically, are the data in the different folds independent of each other, explain why this is important.","metadata":{"cell_id":"00027-daa2a96a-1f67-41a8-a88a-3acf53d2e228","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00028-8ee9ba96-765c-4ece-8ae1-2707aa77e7a1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nUsing a `for` loop with the `KFold` generator it is then possible to create test and train subsets, fit the model of interest and calculate the training and testing metrics but this requires a fair bit of book keeping code to implement, see an example of this [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n\nHowever, `sklearn` provides a more convenient way of doing all of this using the  `cross_val_score` function from the `model_selection` submodule. This function takes as arguments our model or pipeline (any object implementing `fit`) and then our full model matrix $X$ and response $y$. The argument `cv` is a cross-validation generator that determines the splitting strategy (e.g. a `KFold` object).","metadata":{"cell_id":"00029-ec2c8ad1-4c94-4bac-8bff-39dc4f729b2e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00030-e994c03a-38a8-4a1a-be93-8b7beba1f130","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.model_selection import cross_val_score\n\nmodel = make_pipeline(\n    PolynomialFeatures(degree=1),\n    LinearRegression(fit_intercept=False)\n)\n\n# Use shuffle to avoid the issue seen w/ Ex. 3 & 4\n# random_state again sets a random seed so we get the same results each time this cell is run\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\ncross_val_score(model, X, y, cv=kf, scoring=\"neg_root_mean_squared_error\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have used `\"neg_root_mean_squared_error\"` as our scoring metric which returns the negative of the root mean squared error. As the name implies this returns the negative of the usual fit metric, this is because sklearn expects to always optimize for the maximum of a score and the model with the largest negative rmse  will therefore be the \"best\". To get a list of all available scoring metrics for `cross_val_score` you can run `sorted(sklearn.metrics.SCORERS.keys())`.\n\nTo obtain these 5-fold CV estimates of rmse for our models we slightly modify our original code as follows,","metadata":{"cell_id":"00031-6b1fd7c2-d945-49aa-9688-b60c9a92977b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00032-a69a71a5-819d-4021-9934-82f34d2c86c2","output_cleared":true,"deepnote_cell_type":"code"},"source":"degree = []\ntest_mean_rmse = []\ntest_rmse = []\n\nM = 30\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\nfor i in np.arange(1,M+1):\n    model = make_pipeline(\n        PolynomialFeatures(degree=i),\n        LinearRegression(fit_intercept=False)\n    )\n    cv = -1 * cross_val_score(model, X, y, cv=kf, scoring=\"neg_root_mean_squared_error\")\n    degree.append(i)\n    test_mean_rmse.append(np.mean(cv))\n    test_rmse.append(cv)\n\ncv = pd.DataFrame(\n    data = np.c_[degree, test_mean_rmse, test_rmse],\n    columns = [\"degree\", \"mean_rmse\"] + [\"fold\" + str(i) for i in range(1,6) ]\n)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This fits $5 \\times 30$ polynomial regression models for and stores the results in the data frame `cv`. The `mean_rmse` column contains the average rmse across all 5 folds.","metadata":{"cell_id":"00033-6188be56-e2de-48fa-a2d9-83d94dec834b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00034-6843abcb-6994-4332-9d0d-5a9348e93ce4","output_cleared":true,"deepnote_cell_type":"code"},"source":"cv.head(n=15)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now plot these data, to assess the different models and their performance on fitting these data.","metadata":{"cell_id":"00035-03d3ad19-6973-4f99-84d2-2fb4085a5ff1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00036-5f536a49-e2a2-41cd-97ca-2316b46664ab","output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.lineplot(x=\"degree\", y=\"mean_rmse\", data = cv, color=\"black\")\nsns.scatterplot(x=\"degree\", y=\"value\", hue=\"variable\", data = pd.melt(cv,id_vars=[\"degree\", \"mean_rmse\"]))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above is a bit hard to read and compare, particularly for the lower degree polynomial models. We can address this by using a log scale for the y-axis.","metadata":{"cell_id":"00037-b8dda437-8456-481d-a19c-b6e94d9a38d2","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00038-651dd95d-5ebc-40cd-9c30-de115aa1324a","output_cleared":true,"deepnote_cell_type":"code"},"source":"g = sns.lineplot(x=\"degree\", y=\"mean_rmse\", data = cv, color=\"black\")\ng = sns.scatterplot(x=\"degree\", y=\"value\", hue=\"variable\", data = pd.melt(cv,id_vars=[\"degree\", \"mean_rmse\"]))\n\ng.set_yscale(\"log\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 5\n\nDo these CV rmse's agree with the results we obtained when using `train_test_split`? How do they differ, is it only a single fold that differs or several?","metadata":{"cell_id":"00039-893f6125-830b-467c-9125-28982d4c76ba","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00040-b3e99ab8-7064-44f9-97dc-1693379f61df","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 6\n\nBased on these results, what value of $M$ do you think produces the best model, justify your answer.","metadata":{"cell_id":"00041-857cdad7-2681-4c5b-9468-37b4d62fe3a1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00042-039457ea-8aaf-471c-a1a9-e245726f456c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 2.2 CV Grid Search\n","metadata":{"cell_id":"00043-c95d8c13-bdc0-4870-9e89-e900d920443c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We can further reduce the amount of code needed if we wish to test over a specific set of parameter values  using cross validation. This is accomplished by using the `GridSearchCV` function from the `model_selection` submodule. \n\nThis function works similarly to `cross_val_score`, but with the addition of the `param_grid` argument. This argument is a dictionary containing parameters names as keys and lists of parameter settings to try as values. Since we are using a pipeline, out parameter name will be the name of the pipeline step, `polynomialfeatures`, followed by `__`, and then the parameter name, `degree`. So for our pipeline the parameter is named `polynomialfeatures__degree`. If you want to list any models available parameters you can call the `get_params()` method on the model object, e.g. `m.get_params()` here.","metadata":{"cell_id":"00044-e95138bd-5ab0-438e-8ad9-7d5a41b468e8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00045-e9da347e-236d-415b-9170-1101b6d4c66c","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.model_selection import GridSearchCV\n\nm = make_pipeline(\n        PolynomialFeatures(),\n        LinearRegression(fit_intercept=False)\n    )\n\nparameters = {\n    'polynomialfeatures__degree': np.arange(1,31,1)\n}\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\ngrid_search = GridSearchCV(m, parameters, cv=kf, scoring=\"neg_root_mean_squared_error\").fit(X, y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above code goes through the process of fitting all $5 \\times 30$ models as well as storing and ranking the results for the requested scoring metric(s).\n\nOnce all of the submodels are fit, we can determine the optimal hyperparameter value by accessing the object's `best_*` attributes,","metadata":{"cell_id":"00046-796b9f2a-55e9-4ee6-add8-9bb0490fe381","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00047-abb82df6-10c3-45cf-a3f0-f3e59e0988aa","output_cleared":true,"deepnote_cell_type":"code"},"source":"print(\"best index: \", grid_search.best_index_)\nprint(\"best param: \", grid_search.best_params_)\nprint(\"best score: \", grid_search.best_score_)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see that this proceedure has selected the model with polynomial degree 13 as having the best fit, and this model achieved an average rmse of $0.196$.\n\nAdditional useful details from the CV process are available in the `cv_results_` attribute, which provides CV and scoring details,","metadata":{"cell_id":"00048-3ff72d90-3254-42be-9cc2-a676a4d725a0","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00049-d6562c8b-0795-4ba0-b49f-aad1b88c9725","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.cv_results_[\"mean_test_score\"]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00050-dcbbfbdb-7761-46e4-bc2a-8298c5f084d8","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.cv_results_[\"split0_test_score\"]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the `best_estimator_` attribute gives direct access to the \"best\" model (pipeline) object. Which allows for direct inspection of model coefficients, make predictions, etc.","metadata":{"cell_id":"00051-3c129219-d7cd-4500-98cb-2f704cd6075e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00052-5f906237-40c9-4dbd-b803-44b7b5541ca8","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00053-573636ec-c073-40c2-9637-39918d20601f","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.best_estimator_.named_steps['linearregression'].coef_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example if we wanted to plot this \"best\" model's fit to the data we can use the object's predict method directly.","metadata":{"cell_id":"00054-77d31192-0a7d-4f97-8d79-2891c1fcc6ae","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00055-1caf990c-bcc8-4ffc-9f3a-76ad398dc2b6","output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.scatterplot(x='x', y='y', data=d, color=\"black\")\nsns.lineplot(\n    x=d.x,\n    y=grid_search.best_estimator_.predict(X)\n)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 7\n\nDo these results appear to be \"robust\"? More specifically, do you think that the degree 13 polynomial model is actually the best? If you were to change the `random_state` used for the shuffling of the folds, will this result change?","metadata":{"cell_id":"00056-acd188c2-2933-41db-b55f-46d723921652","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00057-ff0fe3bb-cce3-4084-9de6-ccb2cbdc70d3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n# 3. Additional dimensions and CV\n\nLet us now consider an additive regression model of the following form,\n\n$$ y = f(x_1) + g(x_2) + h(x_3) + \\epsilon $$\n\nwhere $f()$, $g()$, and $h()$ are polynomials with fix degrees, we will assume linear, quadratic and cubic in this case with the following coefficients:\n\n$$\n\\begin{aligned}\nf(x) &= 1.2 x + 1.1 \\\\\ng(x) &= 2.5 x^2 - 0.9 x - 3.2  \\\\\nh(x) &= 2 x^3 + 0.4 x^2 - 5.2 x + 2.7\n\\end{aligned}\n$$\n\nWe generate values for $x_1$, $x_2$, $x_3$, and $\\epsilon$ and then use these values to calculate observations of $y$ using the following code.\n","metadata":{"cell_id":"00058-b81b792b-3b71-4614-8339-d5f365a7734c","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00059-61dfe155-ff5d-4489-8012-e1aaf4dd0934","output_cleared":true,"deepnote_cell_type":"code"},"source":"np.random.seed(1234)\nn = 500\n\nf = lambda x: 1.2 * x + 1.1\ng = lambda x: 2.5 * x**2 - 0.9 * x - 3.2 \nh = lambda x: 2 * x**3 + 0.4 * x**2 - 5.2 * x + 2.7\n\nex2 = pd.DataFrame({\n    \"x1\": np.random.rand(n),\n    \"x2\": np.random.rand(n),\n    \"x3\": np.random.rand(n)\n}).assign(\n   y = lambda d: f(d.x1) + g(d.x2) + h(d.x3) + 0.25*np.random.randn(n) # epsilon\n)\n\nprint(ex2)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 8\n\nCreate a pairs plot of these data, from this alone is it possible to identify the polynomial relationships between $y$ and the $x$s?","metadata":{"cell_id":"00060-35cf3a8a-f874-48f9-859e-92c1e2c6884b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00061-45c01f8b-a3ca-490c-bb7e-37ea571b1ee6","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00062-420273ae-dd1f-46f0-8413-225ef493f906","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nWe will assume that we know that each of the functions $f()$, $g()$, and $h()$ are at most of degree 3 - we can try to fit a polynomial model to these data using the tools we've seen thus far. ","metadata":{"cell_id":"00063-121ac542-b680-467d-8b37-74548312cc8d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00064-58bd31cf-ced6-4923-8023-f0eb811a676a","output_cleared":true,"deepnote_cell_type":"code"},"source":"X = ex2.drop(columns=['y']) # Keep as a data frame not a nparray\ny = ex2.y","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00065-eedf918c-1114-4d47-b833-beb6f5d02467","output_cleared":true,"deepnote_cell_type":"code"},"source":"m = make_pipeline(\n    PolynomialFeatures(degree=3),\n    LinearRegression(fit_intercept=False)\n)\n\nfit = m.fit(X, y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following line prints out the model coefficients for the fitted model. While we may have expected only 10 (or 9) parameters in this model, we instead get 20 coefficients for this model.","metadata":{"cell_id":"00066-c9e6805d-ee14-48a3-8e4d-aa398ca444b5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00067-55887400-ff15-4096-867a-057f44da3fda","output_cleared":true,"deepnote_cell_type":"code"},"source":"print( fit.named_steps['linearregression'].coef_ )","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason for this becomes more clear if we examine the `powers_` attribute which returns lists of the powers of $x_1$, $x_2$, and $x_3$ for each of the model terms. So for example the coeficient $0.282$ belongs to the $x_1^0 \\, x_2^0 \\, x_3^0$ term (i.e. the model intercept). ","metadata":{"cell_id":"00068-775cab03-ceba-4b55-b4fb-8f2531b341e2","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00069-68233f33-3caf-4b45-ad75-c3b4e6c651a9","output_cleared":true,"deepnote_cell_type":"code"},"source":"print( fit.named_steps['polynomialfeatures'].powers_ )","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 10\n\nCompare the fitted coefficients with the \"true\" values for our data, how close are they?","metadata":{"cell_id":"00070-19bed0bd-81c2-46c2-b3e4-9a0c8f46e2d9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00071-31bdc88b-62f8-46b3-9e56-7c73317374f0","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 11\n\nCalculate the 5-fold cross validation rmse for this model.","metadata":{"cell_id":"00072-e9ef7b8d-4d41-49c9-8ecd-82fdf579cda7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00073-2f134b8f-d30d-4bcb-9450-ca90e1595b85","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n## 3.2 Column Transformers\n\nFor this particular model we do not want these interaction terms between our features, but this is not something that `sklearn` allows us to disable for `PolynomialFeatures`. This is actually a specific example of a more general issue where we do not want to apply a transformer to all of the features of a model at the same time. For this particularly example, we would like to apply an individual polynomial transformation to each of the three $x$. To do this we will use sklearn's `ColumnTransformer` and the `make_column_transformer` helper function from the `compose` submodule.","metadata":{"cell_id":"00074-6239fcde-9583-4437-9188-9773672683c5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00075-079592d1-a401-439c-a589-fa7b175bd301","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.compose import ColumnTransformer, make_column_transformer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00076-c5d1484e-066f-4316-a666-55be6da5af8b","output_cleared":true,"deepnote_cell_type":"code"},"source":"ind_poly = make_column_transformer(\n    (PolynomialFeatures(degree=3, include_bias=False), ['x1']),\n    (PolynomialFeatures(degree=3, include_bias=False), ['x2']),\n    (PolynomialFeatures(degree=3, include_bias=False), ['x3']),\n)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The arguments for `make_column_transformer` are tuples of the desired transformer object and the name of the columns that will have that transformer applied. Once constructed the column transfomer works like any other transformer object and can be applied via `fit` and `transform` or `fit_transform` methods.","metadata":{"cell_id":"00077-7b082005-ac52-40fc-9668-80028be94ea8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00078-1724bf64-e667-43e7-938a-5beb101b1315","output_cleared":true,"deepnote_cell_type":"code"},"source":"trans = ind_poly.fit_transform(X, y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00079-79426d1b-97d9-4cbf-8b5d-6a13de55c135","output_cleared":true,"deepnote_cell_type":"code"},"source":"pd.DataFrame(trans) # printing as a DataFrame makes the array more readable","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`ColumnTransformer`s are like `pipeline`s but they include a specific column or columns for the transformer to be applied. By using this transformer we take each feature and apply a single polynomial feature transformer, of degree 3 (excluding the intercept column (bias)), resulting in 9 total features as output (3 for each input feature). We can check these values make sense by examining them along with the original values of the $x$s. Here we are using `include_bias=False` to avoid creating a rank deficient model matrix, which would result if all three polynomial features transforms included the same intercept column.","metadata":{"cell_id":"00080-295e71f0-1654-42d2-bc4c-19662ce60c76","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00081-5d42178d-4345-411e-8063-1d2d097881b2","output_cleared":true,"deepnote_cell_type":"code"},"source":"pd.concat([X, pd.DataFrame(trans)], axis=1)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A `ColumnTransformer` is like any other transformer and can therefore be included in a pipeline, this enables us to create a pipeline for fitting our desired polynomial regression model (with no interaction terms). Since the polynomial features no longer include an intercept, we can add this back to the model with `fit_intercept=True` in the linear regression step.","metadata":{"cell_id":"00082-d0a10e37-a5d8-4710-9079-2af9e670eeef","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00083-b3b392dc-3583-4641-8e70-dd13b80d34af","output_cleared":true,"deepnote_cell_type":"code"},"source":"m2 = make_pipeline(\n    make_column_transformer(\n        (PolynomialFeatures(degree=3, include_bias=False), ['x1']),\n        (PolynomialFeatures(degree=3, include_bias=False), ['x2']),\n        (PolynomialFeatures(degree=3, include_bias=False), ['x3']),\n    ),\n    LinearRegression(fit_intercept=True)\n)\n\nfit = m2.fit(X, y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can examine the fitted values of the coefficients by accessing the `linearregression` step and its `coef_` and `intercept_` attributes.","metadata":{"cell_id":"00084-3b1ebab4-b0bc-4162-a5b4-d9560d2e74d2","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00085-87ebbf54-19e9-4213-8248-ccec75a2c9ad","output_cleared":true,"deepnote_cell_type":"code"},"source":"fit.named_steps['linearregression'].coef_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00086-5913ba8d-a2e5-4d8d-af7b-c1c035476d53","output_cleared":true,"deepnote_cell_type":"code"},"source":"fit.named_steps['linearregression'].intercept_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of directly fitting, we can also use this pipeline with cross validation functions like `cross_val_score` to obtain a more reliable estimate of our model's rmses.","metadata":{"cell_id":"00087-c4e5a526-ad00-49ef-84d3-48d4b460dd25","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00088-3a9351c3-83e5-4203-b590-be2a85af00e0","output_cleared":true,"deepnote_cell_type":"code"},"source":"cv = cross_val_score(m2, X, y, cv=5, scoring=\"neg_root_mean_squared_error\")\n\nprint(cv)\nprint(cv.mean())","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 12\n\nIs this rmse better or worse than the rmse calculated for the original model that included interactions? Explain why you think this is.","metadata":{"cell_id":"00089-69aa3093-17cf-4169-b46e-c2d8740cbcfd","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00090-3961e74e-b1d7-42b1-8789-918d8ace2f79","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 3.3 Column Transformers & CV Grid Search\n\nFinally we will see if we can come close to recovering the original forms of $f()$, $g()$, and $h()$ using `GridSearchCV`.  This builds on our previous use of this function, but now we need to optimize over the degree parameter of all three of the polynomial feature transformers. We can examine the names of these transforms by examining the `named_transformers_` attribute associated with the `columntransformer`,","metadata":{"cell_id":"00091-0fc2eaf8-6ed1-40f0-b01e-8af7d5a8aad9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00092-c94864c0-4d94-4e57-9ddb-be5c253513bf","output_cleared":true,"deepnote_cell_type":"code"},"source":"m2.named_steps['columntransformer'].named_transformers_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us the transformer names: `polynomialfeatures-1`, `polynomialfeatures-2`, and `polynomialfeatures-3` which are referenced in the same way by combining the step name with the transformer name and then the parameter name separated by `__`. As such, the degree parameter for the first transformer will be `columntransformer__polynomialfeatures-1__degree`. It is also possible to view all of the parameters for a model or pipeline by looking at the keys returns by the `get_params` method.","metadata":{"cell_id":"00093-16e755ab-01db-405c-aec0-5584b928c902","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00094-58cf93ba-82c3-4b94-9c20-50c7cd992317","output_cleared":true,"deepnote_cell_type":"code"},"source":"m2.get_params().keys()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To keep the space of parameters being explored reasonable we will restrict the possible value of the degrees parameter to be in $[1,\\ldots,5]$. This may take a little bit as we are now fitting a decently large number of models.","metadata":{"cell_id":"00095-18161060-1971-4d93-b5c4-97e09ff67f50","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00096-98000146-9621-4249-a6b2-04d226605425","output_cleared":true,"deepnote_cell_type":"code"},"source":"parameters = {\n    'columntransformer__polynomialfeatures-1__degree': np.arange(1,5,1),\n    'columntransformer__polynomialfeatures-2__degree': np.arange(1,5,1),\n    'columntransformer__polynomialfeatures-3__degree': np.arange(1,5,1),\n}\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\ngrid_search = GridSearchCV(m2, parameters, cv=kf, scoring=\"neg_root_mean_squared_error\").fit(X, y)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 13\n\nHow many models have been fit and scored by `GridSearchCV`?","metadata":{"cell_id":"00097-ac45301c-6bbd-4f63-9640-b5db92963df1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00098-ea57bfd8-f37f-41ba-b2f6-98b0ee5a0258","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nOnce fit, we can determine the optimal parameter value by accessing `grid_search`'s attributes,","metadata":{"cell_id":"00099-373d648f-aa87-4b00-9722-2e39d2ea6708","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00100-8e303b0b-dc5b-4a29-a711-722a26a52041","output_cleared":true,"deepnote_cell_type":"code"},"source":"print(\"best index: \", grid_search.best_index_)\nprint(\"best param: \", grid_search.best_params_)\nprint(\"best score: \", grid_search.best_score_)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 14\n\nBased on these results have we done a good job of recovering the general structure of the functions $f()$, $g()$, and $h()$? e.g. have we correctly recovered the degrees of these functions.","metadata":{"cell_id":"00101-faad4c5a-3516-410a-a300-12793af8e755","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00102-33f82b64-a027-4ace-b4d1-72ae32ece948","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nWe can directly access the properties of the \"best\" model, according to our scoring method, using the `best_estimator_` attribute. From this we can access the `linearregression` step of the pipeline to recover the model coefficients.","metadata":{"cell_id":"00103-38f49527-036d-4328-8b93-0084bb27a402","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00104-a21f4363-ee5b-4220-8cab-58e61ec78b7c","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.best_estimator_.named_steps[\"linearregression\"].intercept_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00105-f94b6ca3-533f-4a61-a504-7354157512fe","output_cleared":true,"deepnote_cell_type":"code"},"source":"grid_search.best_estimator_.named_steps[\"linearregression\"].coef_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 15\n\nCompare the coefficient values we obtained via `GridSearchCV` to the true values used to generate the $y$ observations, how well have recovered the truth values of the coefficients?","metadata":{"cell_id":"00106-5da3164f-bbbc-4237-b3c2-3270f25aa6f2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00107-51406b38-361f-4a07-807f-bd37b55f472f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Bonus Exercise\n\nRepeat the analysis above but use only 200 observations of `ex2` instead of all 500. How does your resulting \"best\" model change? What about 100 or 50 observations? How dependent are the results on the original sample size?","metadata":{"cell_id":"00108-da392bde-c3ba-4969-a4e3-c3c1b91cb311","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 4. Competing the worksheet\n\nAt this point you have hopefully been able to complete all the preceeding exercises. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF and turn it in on gradescope under the `mlp-week04` assignment.","metadata":{"cell_id":"00109-b5bf2493-267b-443f-a0a6-7062d091d5ff","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00112-0fc6faf7-b9f4-426b-a462-726a2601e149","deepnote_to_be_reexecuted":false,"execution_millis":16147,"source_hash":"950a0210","tags":[],"output_cleared":true,"execution_start":1612354252034,"deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp-week04.ipynb","execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"00111-ad9a9fd0-fcc2-4ab1-a75d-12d836e496e7","output_cleared":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":4,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"5023007d-9528-4a57-b3df-4db60e744e23","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}}}