# Display plots inline
get_ipython().run_line_magic("matplotlib", " inline")

# Data libraries
import pandas as pd
import numpy as np

# Plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Plotting defaults
plt.rcParams['figure.figsize'] = (8,5)
plt.rcParams['figure.dpi'] = 80

# sklearn modules
import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline


d = pd.read_csv("gp.csv")
n = d.shape[0] # number of rows

sns.scatterplot(x='x', y='y', data=d, color="black")


from sklearn.model_selection import train_test_split

X = np.c_[d.x]
y = d.y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)


print("orig sizes :", X.shape, y.shape)
print("train sizes:", X_train.shape, y_train.shape)
print("test sizes :", X_test.shape, y_test.shape)


degree = []
train_rmse = []
test_rmse = []

M = 30

for i in np.arange(1, M+1):
    m = make_pipeline(
        PolynomialFeatures(degree=i),
        LinearRegression(fit_intercept=False)
    ).fit(X_train, y_train)
    
    degree.append(i)
    train_rmse.append(mean_squared_error(y_train, m.predict(X_train), squared=False))
    test_rmse.append(mean_squared_error(y_test, m.predict(X_test), squared=False))

fit = pd.DataFrame(data = {"degree": degree, "train_rmse": train_rmse, "test_rmse": test_rmse})


sns.lineplot(x="degree", y="value", hue="variable", data = pd.melt(fit,id_vars=["degree"]))


from sklearn.model_selection import KFold


kf = KFold(n_splits=5)


kf.split(X, y)


[train for train, test in kf.split(X, y)]


[test for train, test in kf.split(X, y)]


from sklearn.model_selection import cross_val_score

model = make_pipeline(
    PolynomialFeatures(degree=1),
    LinearRegression(fit_intercept=False)
)

# Use shuffle to avoid the issue seen w/ Ex. 3 & 4
# random_state again sets a random seed so we get the same results each time this cell is run
kf = KFold(n_splits=5, shuffle=True, random_state=0)
cross_val_score(model, X, y, cv=kf, scoring="neg_root_mean_squared_error")


degree = []
test_mean_rmse = []
test_rmse = []

M = 30
kf = KFold(n_splits=5, shuffle=True, random_state=0)

for i in np.arange(1,M+1):
    model = make_pipeline(
        PolynomialFeatures(degree=i),
        LinearRegression(fit_intercept=False)
    )
    cv = -1 * cross_val_score(model, X, y, cv=kf, scoring="neg_root_mean_squared_error")
    degree.append(i)
    test_mean_rmse.append(np.mean(cv))
    test_rmse.append(cv)

cv = pd.DataFrame(
    data = np.c_[degree, test_mean_rmse, test_rmse],
    columns = ["degree", "mean_rmse"] + ["fold" + str(i) for i in range(1,6) ]
)


cv.head(n=15)


sns.lineplot(x="degree", y="mean_rmse", data = cv, color="black")
sns.scatterplot(x="degree", y="value", hue="variable", data = pd.melt(cv,id_vars=["degree", "mean_rmse"]))


g = sns.lineplot(x="degree", y="mean_rmse", data = cv, color="black")
g = sns.scatterplot(x="degree", y="value", hue="variable", data = pd.melt(cv,id_vars=["degree", "mean_rmse"]))

g.set_yscale("log")


from sklearn.model_selection import GridSearchCV

m = make_pipeline(
        PolynomialFeatures(),
        LinearRegression(fit_intercept=False)
    )

parameters = {
    'polynomialfeatures__degree': np.arange(1,31,1)
}

kf = KFold(n_splits=5, shuffle=True, random_state=0)

grid_search = GridSearchCV(m, parameters, cv=kf, scoring="neg_root_mean_squared_error").fit(X, y)


print("best index: ", grid_search.best_index_)
print("best param: ", grid_search.best_params_)
print("best score: ", grid_search.best_score_)


grid_search.cv_results_["mean_test_score"]


grid_search.cv_results_["split0_test_score"]


grid_search.best_estimator_


grid_search.best_estimator_.named_steps['linearregression'].coef_


sns.scatterplot(x='x', y='y', data=d, color="black")
sns.lineplot(
    x=d.x,
    y=grid_search.best_estimator_.predict(X)
)


np.random.seed(1234)
n = 500

f = lambda x: 1.2 * x + 1.1
g = lambda x: 2.5 * x**2 - 0.9 * x - 3.2 
h = lambda x: 2 * x**3 + 0.4 * x**2 - 5.2 * x + 2.7

ex2 = pd.DataFrame({
    "x1": np.random.rand(n),
    "x2": np.random.rand(n),
    "x3": np.random.rand(n)
}).assign(
   y = lambda d: f(d.x1) + g(d.x2) + h(d.x3) + 0.25*np.random.randn(n) # epsilon
)

print(ex2)





X = ex2.drop(columns=['y']) # Keep as a data frame not a nparray
y = ex2.y


m = make_pipeline(
    PolynomialFeatures(degree=3),
    LinearRegression(fit_intercept=False)
)

fit = m.fit(X, y)


print( fit.named_steps['linearregression'].coef_ )


print( fit.named_steps['polynomialfeatures'].powers_ )








from sklearn.compose import ColumnTransformer, make_column_transformer


ind_poly = make_column_transformer(
    (PolynomialFeatures(degree=3, include_bias=False), ['x1']),
    (PolynomialFeatures(degree=3, include_bias=False), ['x2']),
    (PolynomialFeatures(degree=3, include_bias=False), ['x3']),
)


trans = ind_poly.fit_transform(X, y)


pd.DataFrame(trans) # printing as a DataFrame makes the array more readable


pd.concat([X, pd.DataFrame(trans)], axis=1)


m2 = make_pipeline(
    make_column_transformer(
        (PolynomialFeatures(degree=3, include_bias=False), ['x1']),
        (PolynomialFeatures(degree=3, include_bias=False), ['x2']),
        (PolynomialFeatures(degree=3, include_bias=False), ['x3']),
    ),
    LinearRegression(fit_intercept=True)
)

fit = m2.fit(X, y)


fit.named_steps['linearregression'].coef_


fit.named_steps['linearregression'].intercept_


cv = cross_val_score(m2, X, y, cv=5, scoring="neg_root_mean_squared_error")

print(cv)
print(cv.mean())


m2.named_steps['columntransformer'].named_transformers_


m2.get_params().keys()


parameters = {
    'columntransformer__polynomialfeatures-1__degree': np.arange(1,5,1),
    'columntransformer__polynomialfeatures-2__degree': np.arange(1,5,1),
    'columntransformer__polynomialfeatures-3__degree': np.arange(1,5,1),
}

kf = KFold(n_splits=5, shuffle=True, random_state=0)

grid_search = GridSearchCV(m2, parameters, cv=kf, scoring="neg_root_mean_squared_error").fit(X, y)


print("best index: ", grid_search.best_index_)
print("best param: ", grid_search.best_params_)
print("best score: ", grid_search.best_score_)


grid_search.best_estimator_.named_steps["linearregression"].intercept_


grid_search.best_estimator_.named_steps["linearregression"].coef_



